package com.yjlee.search.evaluation.service;

import co.elastic.clients.elasticsearch.ElasticsearchClient;
import co.elastic.clients.elasticsearch.core.ClearScrollRequest;
import co.elastic.clients.elasticsearch.core.ScrollRequest;
import co.elastic.clients.elasticsearch.core.ScrollResponse;
import co.elastic.clients.elasticsearch.core.SearchRequest;
import co.elastic.clients.elasticsearch.core.SearchResponse;
import co.elastic.clients.elasticsearch.core.search.Hit;
import com.yjlee.search.common.util.PromptTemplateLoader;
import com.yjlee.search.evaluation.dto.GenerateQueryRequest;
import com.yjlee.search.evaluation.dto.ProductInfoDto;
import com.yjlee.search.evaluation.model.EvaluationQuery;
import com.yjlee.search.evaluation.repository.EvaluationQueryRepository;
import com.yjlee.search.index.dto.ProductDocument;
import com.yjlee.search.search.constants.ESFields;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.stream.Collectors;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

@Slf4j
@Service
@RequiredArgsConstructor
public class QueryGenerationService {

  private static final int BATCH_SIZE = 50;
  private static final int SPECS_MAX_LENGTH = 300;
  private static final String QUERY_TEMPLATE_FILE = "query-generation.txt";
  private static final String SCROLL_TIMEOUT = "5m";
  private static final String LLM_LOG_DIR = "logs/llm";

  private final ElasticsearchClient elasticsearchClient;
  private final EvaluationQueryRepository evaluationQueryRepository;
  private final LLMService llmService;
  private final PromptTemplateLoader promptTemplateLoader;

  public void generateCandidateQueries(GenerateQueryRequest request) {
    try {
      log.info("ğŸš€ ì¿¼ë¦¬ ìƒì„± ì‹œì‘");
      ensureLLMLogDirectory();

      List<ProductInfoDto> allProducts = fetchAllProducts();
      log.info("ğŸ“Š Scroll APIë¡œ ìƒí’ˆ ì¡°íšŒ ì™„ë£Œ: {}ê°œ", allProducts.size());

      Map<String, Integer> queryCountMap = new HashMap<>();
      List<List<ProductInfoDto>> batches = new ArrayList<>();
      for (int i = 0; i < allProducts.size(); i += BATCH_SIZE) {
        int end = Math.min(i + BATCH_SIZE, allProducts.size());
        batches.add(allProducts.subList(i, end));
      }

      var executor = Executors.newFixedThreadPool(10);
      List<Future<Map<String, Integer>>> futures = new ArrayList<>();
      for (List<ProductInfoDto> batch : batches) {
        futures.add(executor.submit(() -> processProductsInBatches(batch)));
      }
      for (var f : futures) {
        try {
          var result = f.get();
          result.forEach((k, v) -> queryCountMap.merge(k, v, Integer::sum));
        } catch (Exception e) {
          log.warn("ì¿¼ë¦¬ ìƒì„± ì¤‘ ì¼ë¶€ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨", e);
        }
      }
      executor.shutdown();
      log.info("ğŸ” ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {}ê°œ ê³ ìœ  ì¿¼ë¦¬", queryCountMap.size());

      saveGeneratedQueries(queryCountMap);
      log.info("âœ… ì¿¼ë¦¬ ì €ì¥ ì™„ë£Œ");

    } catch (Exception e) {
      log.error("âŒ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨", e);
      throw new RuntimeException("ì¿¼ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", e);
    }
  }

  private void ensureLLMLogDirectory() {
    try {
      Path logDir = Paths.get(LLM_LOG_DIR);
      if (!Files.exists(logDir)) {
        Files.createDirectories(logDir);
        log.info("ğŸ“ LLM ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±: {}", logDir.toAbsolutePath());
      }
    } catch (IOException e) {
      log.warn("âš ï¸ LLM ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨", e);
    }
  }

  private List<ProductInfoDto> fetchAllProducts() {
    List<ProductInfoDto> allProducts = new ArrayList<>();
    String scrollId = null;

    try {
      // ì²« ë²ˆì§¸ scroll ê²€ìƒ‰ ìš”ì²­
      SearchResponse<ProductDocument> initialResponse = initiateScrollSearch();
      scrollId = initialResponse.scrollId();

      // ì²« ë²ˆì§¸ ë°°ì¹˜ ì²˜ë¦¬
      List<ProductInfoDto> batch = extractProductInfoFromHits(initialResponse.hits().hits());
      allProducts.addAll(batch);
      log.info("ğŸ“Š ì²« ë²ˆì§¸ ë°°ì¹˜ ì¡°íšŒ: {}ê°œ", batch.size());

      // scrollì„ í†µí•´ ë‚˜ë¨¸ì§€ ë°°ì¹˜ë“¤ ì²˜ë¦¬
      while (true) {
        ScrollResponse<ProductDocument> scrollResponse = continueScroll(scrollId);

        if (scrollResponse.hits().hits().isEmpty()) {
          break;
        }

        batch = extractProductInfoFromHits(scrollResponse.hits().hits());
        allProducts.addAll(batch);
        log.debug("ğŸ“Š ë°°ì¹˜ ì¶”ê°€ ì¡°íšŒ: {}ê°œ (ì´ {}ê°œ)", batch.size(), allProducts.size());

        scrollId = scrollResponse.scrollId();
      }

    } catch (Exception e) {
      log.error("âŒ Scroll ê²€ìƒ‰ ì‹¤íŒ¨", e);
    } finally {
      // scroll ì •ë¦¬
      if (scrollId != null) {
        clearScroll(scrollId);
      }
    }

    return allProducts;
  }

  private SearchResponse<ProductDocument> initiateScrollSearch() throws Exception {
    SearchRequest searchRequest =
        SearchRequest.of(
            builder ->
                builder
                    .index(ESFields.PRODUCTS_SEARCH_ALIAS)
                    .size(BATCH_SIZE)
                    .scroll(s -> s.time(SCROLL_TIMEOUT))
                    .source(
                        src ->
                            src.filter(
                                f ->
                                    f.includes(
                                        ESFields.PRODUCT_NAME_RAW, ESFields.PRODUCT_SPECS_RAW)))
                    .query(q -> q.term(t -> t.field(ESFields.CATEGORY_NAME).value("SSD"))));

    log.debug("ğŸ” Scroll ê²€ìƒ‰ ì‹œì‘: batch_size={}, timeout={}", BATCH_SIZE, SCROLL_TIMEOUT);
    return elasticsearchClient.search(searchRequest, ProductDocument.class);
  }

  private ScrollResponse<ProductDocument> continueScroll(String scrollId) throws Exception {
    ScrollRequest scrollRequest =
        ScrollRequest.of(builder -> builder.scrollId(scrollId).scroll(s -> s.time(SCROLL_TIMEOUT)));

    return elasticsearchClient.scroll(scrollRequest, ProductDocument.class);
  }

  private void clearScroll(String scrollId) {
    try {
      ClearScrollRequest clearRequest =
          ClearScrollRequest.of(builder -> builder.scrollId(scrollId));
      elasticsearchClient.clearScroll(clearRequest);
      log.debug("ğŸ§¹ Scroll ì •ë¦¬ ì™„ë£Œ: scrollId={}", scrollId);
    } catch (Exception e) {
      log.warn("âš ï¸ Scroll ì •ë¦¬ ì‹¤íŒ¨: scrollId={}", scrollId, e);
    }
  }

  private List<ProductInfoDto> extractProductInfoFromHits(List<Hit<ProductDocument>> hits) {
    return hits.stream()
        .filter(hit -> isValidProduct(hit.source()))
        .map(
            hit ->
                new ProductInfoDto(
                    hit.id(), hit.source().getNameRaw().trim(), hit.source().getSpecsRaw()))
        .collect(Collectors.toList());
  }

  private boolean isValidProduct(ProductDocument product) {
    return product != null
        && product.getNameRaw() != null
        && !product.getNameRaw().trim().isEmpty();
  }

  private Map<String, Integer> processProductsInBatches(List<ProductInfoDto> allProducts) {
    List<List<ProductInfoDto>> batches = createBatches(allProducts);
    Map<String, Integer> aggregatedQueries = new HashMap<>();

    for (int i = 0; i < batches.size(); i++) {
      Map<String, Integer> batchQueries = processSingleBatch(batches.get(i), i + 1, batches.size());
      mergeQueryCounts(aggregatedQueries, batchQueries);
    }

    return aggregatedQueries;
  }

  private List<List<ProductInfoDto>> createBatches(List<ProductInfoDto> products) {
    List<List<ProductInfoDto>> batches = new ArrayList<>();
    for (int i = 0; i < products.size(); i += BATCH_SIZE) {
      int endIndex = Math.min(i + BATCH_SIZE, products.size());
      batches.add(products.subList(i, endIndex));
    }
    return batches;
  }

  private Map<String, Integer> processSingleBatch(
      List<ProductInfoDto> batch, int batchNumber, int totalBatches) {
    try {
      log.debug("ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {}/{} (ìƒí’ˆ {}ê°œ)", batchNumber, totalBatches, batch.size());
      return generateQueriesFromLLM(batch);
    } catch (Exception e) {
      log.error("âŒ ë°°ì¹˜ {} ì²˜ë¦¬ ì‹¤íŒ¨", batchNumber, e);
      return Collections.emptyMap();
    }
  }

  private Map<String, Integer> generateQueriesFromLLM(List<ProductInfoDto> products) {
    String prompt = buildPromptForProducts(products);

    // LLM API í˜¸ì¶œ
    String llmResponse = llmService.callLLMAPI(prompt);

    // ì…ë ¥/ì¶œë ¥ íŒŒì¼ë¡œ ì €ì¥
    saveLLMInteraction(prompt, llmResponse, products.size());

    if (isEmptyResponse(llmResponse)) {
      log.warn("âš ï¸ LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŒ");
      return Collections.emptyMap();
    }

    return parseQueriesFromResponse(llmResponse, products.size());
  }

  private void saveLLMInteraction(String prompt, String response, int productCount) {
    try {
      String timestamp =
          LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss_SSS"));
      String baseFileName = String.format("batch_%s_products_%d", timestamp, productCount);

      // ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì €ì¥
      Path promptFile = Paths.get(LLM_LOG_DIR, baseFileName + "_prompt.txt");
      Files.writeString(promptFile, prompt, StandardOpenOption.CREATE);

      // ì¶œë ¥ ì‘ë‹µ ì €ì¥
      Path responseFile = Paths.get(LLM_LOG_DIR, baseFileName + "_response.txt");
      Files.writeString(
          responseFile, response != null ? response : "[NULL_RESPONSE]", StandardOpenOption.CREATE);

      log.debug(
          "ğŸ’¾ LLM ìƒí˜¸ì‘ìš© ì €ì¥: prompt={}, response={}",
          promptFile.getFileName(),
          responseFile.getFileName());

    } catch (IOException e) {
      log.warn("âš ï¸ LLM ìƒí˜¸ì‘ìš© ì €ì¥ ì‹¤íŒ¨", e);
    }
  }

  private String buildPromptForProducts(List<ProductInfoDto> products) {
    StringBuilder productListText = new StringBuilder();
    for (int i = 0; i < products.size(); i++) {
      productListText.append(String.format("%d. %s", i, formatProductForPrompt(products.get(i))));
      if (i < products.size() - 1) {
        productListText.append("\n");
      }
    }

    Map<String, String> variables = Map.of("PRODUCT_LIST", productListText.toString());
    return promptTemplateLoader.loadTemplate(QUERY_TEMPLATE_FILE, variables);
  }

  private String formatProductForPrompt(ProductInfoDto product) {
    StringBuilder formatted = new StringBuilder();
    formatted.append(String.format("ìƒí’ˆëª…: %s", product.getNameRaw()));

    if (hasValidSpecs(product)) {
      String specs = truncateSpecs(product.getSpecsRaw());
      formatted.append(String.format("\n   ìŠ¤í™: %s", specs));
    }

    return formatted.toString();
  }

  private boolean hasValidSpecs(ProductInfoDto product) {
    return product.getSpecsRaw() != null && !product.getSpecsRaw().trim().isEmpty();
  }

  private String truncateSpecs(String specs) {
    return specs.length() > SPECS_MAX_LENGTH ? specs.substring(0, SPECS_MAX_LENGTH) + "..." : specs;
  }

  private boolean isEmptyResponse(String response) {
    return response == null || response.trim().isEmpty();
  }

  private Map<String, Integer> parseQueriesFromResponse(String response, int expectedProductCount) {
    try {
      Map<String, Integer> queryCount = new HashMap<>();
      String[] lines = response.split("\\r?\\n");

      int processedLines = 0;
      for (String line : lines) {
        if (line.trim().isEmpty() || processedLines >= expectedProductCount) {
          continue;
        }

        List<String> queries = extractQueriesFromLine(line);
        queries.forEach(query -> queryCount.merge(query, 1, Integer::sum));

        processedLines++;
      }

      return queryCount;
    } catch (Exception e) {
      log.error("âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨", e);
      return Collections.emptyMap();
    }
  }

  private List<String> extractQueriesFromLine(String line) {
    return Arrays.stream(line.split("\\t"))
        .map(String::trim)
        .filter(query -> !query.isEmpty())
        .collect(Collectors.toList());
  }

  private void mergeQueryCounts(Map<String, Integer> target, Map<String, Integer> source) {
    source.forEach((query, count) -> target.merge(query, count, Integer::sum));
  }

  private void saveGeneratedQueries(Map<String, Integer> queryCountMap) {
    List<EvaluationQuery> evaluationQueries =
        queryCountMap.entrySet().stream()
            .map(this::createEvaluationQuery)
            .collect(Collectors.toList());

    evaluationQueryRepository.saveAll(evaluationQueries);
  }

  private EvaluationQuery createEvaluationQuery(Map.Entry<String, Integer> entry) {
    return EvaluationQuery.builder().query(entry.getKey()).count(entry.getValue()).build();
  }
}
